# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XtxUFrsgi1aRbhePkN65BI5nSzVNvh97
"""

# Colab cell: install libs (run)
# NOTE: torch wheel may depend on CUDA in the VM. If you get CUDA/torch errors,
# see the comments below or run a matching torch + CUDA wheel.
!pip install -q transformers accelerate safetensors bitsandbytes huggingface_hub
!pip install -q "uvicorn[standard]" fastapi streamlit pyngrok requests

# Try to install a common torch+cuda wheel (this usually works on Colab GPU)
# If this errors, remove this line and install the proper wheel for your CUDA.
!pip install -q torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118

# Colab cell: login to HF
from huggingface_hub import login
from getpass import getpass
token = getpass("Paste your Hugging Face token here: ")
login(token)
print("Logged in.")

# Colab cell: load model + tokenizer (run; may take a few minutes)
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch, os

MODEL_NAME = "ibm-granite/granite-3.3-2b-instruct"

# pick dtype: prefer bfloat16 on A100/H100 (big speed/memory win), else float16
if torch.cuda.is_available():
    dev = torch.device("cuda")
    # heuristic: newer Ampere/Next GPUs support bf16. If not sure, use float16.
    try:
        # safe fallback: many Colab GPUs won't support bf16, so use float16
        torch_dtype = torch.bfloat16 if torch.cuda.get_device_properties(0).major >= 8 else torch.float16
    except Exception:
        torch_dtype = torch.float16
else:
    dev = torch.device("cpu")
    torch_dtype = torch.float32

print("device:", dev, "torch_dtype:", torch_dtype)

# load tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)

# load model: device_map='auto' will put weights on GPU if available.
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map="auto",
    torch_dtype=torch_dtype,
    low_cpu_mem_usage=True
)

print("Model loaded. Ready.")

# Colab cell: inference helper
from transformers import set_seed

def generate_reply(user_text, max_new_tokens=256, thinking=False, seed=42):
    set_seed(seed)
    conv = [{"role": "user", "content": user_text}]
    # apply model's chat template (model supports this)
    inputs = tokenizer.apply_chat_template(
        conv,
        return_tensors="pt",
        thinking=thinking,
        return_dict=True,
        add_generation_prompt=True
    ).to(model.device)
    out = model.generate(**inputs, max_new_tokens=max_new_tokens)
    # decode only the newly generated tokens
    reply = tokenizer.decode(out[0, inputs["input_ids"].shape[1]:], skip_special_tokens=True)
    return reply

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# cat > server.py <<'PY'
# from fastapi import FastAPI
# from pydantic import BaseModel
# import asyncio
# import os
# 
# # If you started model in the notebook and want to reuse it, you can adapt this file.
# # For simplicity this script will import the same helper if placed in same folder.
# # We'll assume tokenizer and model are available via import in this environment.
# 
# from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed
# import torch
# 
# MODEL_NAME = "ibm-granite/granite-3.3-2b-instruct"
# 
# # LOAD MODEL (this happens once when server starts)
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# torch_dtype = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.get_device_properties(0).major >= 8) else (torch.float16 if torch.cuda.is_available() else torch.float32)
# 
# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)
# model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map="auto", torch_dtype=torch_dtype, low_cpu_mem_usage=True)
# 
# def sync_generate(user_text, max_new_tokens=256, thinking=False, seed=42):
#     set_seed(seed)
#     conv = [{"role":"user","content":user_text}]
#     inputs = tokenizer.apply_chat_template(conv, return_tensors="pt", thinking=thinking, return_dict=True, add_generation_prompt=True).to(model.device)
#     out = model.generate(**inputs, max_new_tokens=max_new_tokens)
#     reply = tokenizer.decode(out[0, inputs["input_ids"].shape[1]:], skip_special_tokens=True)
#     return reply
# 
# app = FastAPI()
# 
# class ChatRequest(BaseModel):
#     message: str
#     thinking: bool = False
#     max_new_tokens: int = 256
# 
# @app.post("/chat")
# async def chat(req: ChatRequest):
#     loop = asyncio.get_event_loop()
#     # run the synchronous generator in threadpool so server stays responsive
#     reply = await loop.run_in_executor(None, sync_generate, req.message, req.max_new_tokens, req.thinking)
#     return {"reply": reply}
# PY
#

# Start FastAPI (server.py) in background
!nohup uvicorn server:app --host 0.0.0.0 --port 8000 --workers 1 &> uvicorn.log &
!echo "FastAPI launched (port 8000)."

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# cat > streamlit_app.py <<'PY'
# import streamlit as st
# import requests
# 
# API_URL = "http://127.0.0.1:8000/chat"
# 
# st.set_page_config(page_title="Personal Finance Chatbot", layout="centered")
# 
# st.title("ðŸ’¬ Personal Finance Chatbot")
# st.caption("Educational demo â€” not financial advice. Please donâ€™t share personal or bank details.")
# 
# if "history" not in st.session_state:
#     st.session_state.history = []
# 
# # --- chat input box at the bottom ---
# for speaker, text in st.session_state.history:
#     if speaker == "You":
#         st.markdown(
#             f"<div style='text-align: right; background-color: #DCF8C6; "
#             f"padding:8px; border-radius: 10px; margin:5px 0; display:inline-block;'>"
#             f"<b>You:</b> {text}</div>",
#             unsafe_allow_html=True,
#         )
#     else:
#         st.markdown(
#             f"<div style='text-align: left; background-color: #F1F0F0; "
#             f"padding:8px; border-radius: 10px; margin:5px 0; display:inline-block;'>"
#             f"<b>Bot:</b> {text}</div>",
#             unsafe_allow_html=True,
#         )
# 
# st.markdown("---")
# 
# # input box
# user_input = st.chat_input("Type your question here...")
# if user_input:
#     st.session_state.history.append(("You", user_input))
#     with st.spinner("Thinking..."):
#         try:
#             r = requests.post(API_URL, json={"message": user_input, "thinking": False})
#             bot_reply = r.json().get("reply", "(no reply)")
#         except Exception as e:
#             bot_reply = f"(error: {e})"
#     st.session_state.history.append(("Bot", bot_reply))
#     st.rerun()
# PY
#

!nohup streamlit run streamlit_app.py --server.port 8501 --server.headless true &> streamlit.log &
!echo "Streamlit launched (port 8501)."

# List running processes
!ps -ef | grep ngrok

# Kill all ngrok processes
!pkill ngrok

from pyngrok import conf,ngrok
conf.get_default().auth_token = "32uoaF9149iIQpjoFeTrXwzUti6_4fAY3ezQwXHr7VMbp3gqr"
public_url = ngrok.connect(8501).public_url
print("Streamlit public URL:", public_url)